# Kafka 2 CockroachDB

## Setup

Bring the Docker Compose up

```bash
docker-compose -f kafka2crdb.yaml up -d
```

Notice in the `kafka-connect` definition how we're installing the latest version of the JDBC Sink Connector and update the JDBC Postgresql Driver.

Make sure everything is up and running

```bash
$ docker-compose -f kafka2crdb.yaml ps
NAME                COMMAND                  SERVICE             STATUS              PORTS
broker              "/etc/confluent/dock…"   broker              running             0.0.0.0:9092->9092/tcp, 0.0.0.0:9101->9101/tcp
cockroach-1         "/cockroach/cockroac…"   cockroach           running             0.0.0.0:8080->8080/tcp, 0.0.0.0:26257->26257/tcp
connect             "bash -c '# Installi…"   kafka-connect       running             0.0.0.0:8083->8083/tcp, 9092/tcp
control-center      "/etc/confluent/dock…"   control-center      running             0.0.0.0:9021->9021/tcp
schema-registry     "/etc/confluent/dock…"   schema-registry     running             0.0.0.0:8081->8081/tcp
zookeeper           "/etc/confluent/dock…"   zookeeper           running             2888/tcp, 0.0.0.0:2181->2181/tcp, 3888/tcp
```

Open the Control Center at <http://localhost:9021> and wait untill the broker shows up as healthy.

Open another tab for the CockroachDB Console at <http://localhost:8080>.

## Demo

We create the below pipeline:

`datagen(source) --> Kafka Topic --> Kafka JDBC Sink Connector --> CockroachDB(target)`

All set, now we are ready to ingest data into this topic.

You can do most of below tasks from within the Control Center.

### Create Kafka topic

Connect to the `broker` container, and crate a topic `transactions` with 4 partitions

```bash
kafka-topics --bootstrap-server broker:9092 --create --topic transactions --partitions 4 
```

Having the topic partitioned allows for multiple task consumers reading from the topic and ingesting data into CockroachDB.

### Configure the Source Connector

Our source will be a data generator.

Open a new terminal, and create the Datagen Connector.

```bash
# the datagen-connect container listens on port 8083
curl -s -X POST http://localhost:8083/connectors/ \
    -H "Content-Type: application/json" -d '{
        "name": "datagen-transactions",
        "config": {
            "connector.class": "io.confluent.kafka.connect.datagen.DatagenConnector",
            "key.converter": "org.apache.kafka.connect.storage.StringConverter",
            "kafka.topic": "transactions",
            "tasks.max": "256",
            "max.interval": "10",
            "quickstart": "transactions"
        }
    }' | jq '.'
```

Confirm in the Control Center that messages are getting generated by visiting the `Topics` section.

### Configure the Sink Connector

We're ready to setup the JDBC Sink Connector to ingest data into CockroachDB.

Note how we set `batch.size` the same as `max.poll.records` to make sure 1 transaction includes only 128 records.

Because we've set `reWriteBatchedInserts=true`, the JDBC Postgres driver will conflate the 128 individual INSERT statements into a single, multi-records INSERT statement.

Note however that this single statement transaction is still an **explicit** transaction.

> _Note: I've built a connector JAR file set with [autocommit=true](https://github.com/confluentinc/kafka-connect-jdbc/blob/master/src/main/java/io/confluent/connect/jdbc/sink/JdbcDbWriter.java#L57) for implicit transactions and it works just fine._

We also set `tasks.max=4` to have 4 consumer processes ingesting data into CockroachDB in parallel. Each task creates its own database connection and reads from a specific topic partition.

```bash
# register the connector 
curl -s -X POST http://localhost:8083/connectors/ \
     -H "Content-Type: application/json" -d '{
        "name": "sink-crdb",
            "config": {
            "connector.class": "io.confluent.connect.jdbc.JdbcSinkConnector",
            "connection.url": "jdbc:postgresql://cockroach-1:26257/defaultdb?reWriteBatchedInserts=true&ApplicationName=txns",
            "topics": "transactions",
            "tasks.max": "4",
            "key.converter": "org.apache.kafka.connect.storage.StringConverter",
            "value.converter": "io.confluent.connect.avro.AvroConverter",
            "value.converter.schema.registry.url": "http://schema-registry:8081",
            "connection.user": "root",
            "connection.password": "",
            "auto.create": true,
            "auto.evolve": true,
            "insert.mode": "insert",
            "pk.mode": "none",
            "pk.fields": "none",
            "batch.size": 256,
            "consumer.override.max.poll.records": 256
            }
    }' | jq '.'
```

### Query data in CockroachDB

In a new Terminal, open a SQL prompt

```bash
docker exec -it cockroach-1 cockroach sql --insecure
```

```sql
-- the table was automatically created by the Kafka JDBC Sink Connector
> SHOW TABLES;
  schema_name |  table_name  | type  | owner | estimated_row_count | locality
--------------+--------------+-------+-------+---------------------+-----------
  public      | transactions | table | root  |                   0 | NULL

-- check the schema of the created table - note how CockroachDB created its own Primary Key
> SHOW CREATE transactions;
   table_name  |                      create_statement
---------------+--------------------------------------------------------------
  transactions | CREATE TABLE public.transactions (
               |     transaction_id INT8 NOT NULL,
               |     card_id INT8 NOT NULL,
               |     user_id STRING NOT NULL,
               |     purchase_id INT8 NOT NULL,
               |     store_id INT8 NOT NULL,
               |     rowid INT8 NOT VISIBLE NOT NULL DEFAULT unique_rowid(),
               |     CONSTRAINT transactions_pkey PRIMARY KEY (rowid ASC)
               | )

             
-- inspect few rows
> SELECT * FROM transactions LIMIT 5;
  transaction_id | card_id | user_id | purchase_id | store_id
-----------------+---------+---------+-------------+-----------
               1 |       7 | User_6  |           0 |        1
               1 |      19 | User_7  |           0 |        6
               1 |      18 | User_5  |           0 |        6
               1 |       6 | User_   |           0 |        3
               1 |      13 | User_   |           0 |        3

-- avoid contention by using AS OF SYSTEM TIME when running large aggregate queries
> SELECT count(*) FROM transactions AS OF SYSTEM TIME '-5s';
  count
----------
  594633

-- wait few seconds, then query again to see count increasing...
> SELECT count(*) FROM transactions AS OF SYSTEM TIME '-5s';
  count
----------
  594992
```

You can view metrics and statement statistics on the DB Console, at <http://localhost:8080>.

## Useful commands

### Kafka commands

Open a terminal and connect to the Kafka broker container

```bash
docker exec -it broker /bin/bash
```

Once in the `broker` container

```bash
# create topic
$ kafka-topics --create --bootstrap-server broker:9092 --topic something
Created topic something.

# optionally, review config
$ kafka-topics --describe --bootstrap-server broker:9092 --topic something
Topic: something   TopicId: Bp-kn4giR920t_U82R_lUw PartitionCount: 1       ReplicationFactor: 1    Configs: 
        Topic: something   Partition: 0    Leader: 1       Replicas: 1     Isr: 1  Offline: 
```

```bash
# reset consumer group offset - need to connect to `broker`
$ kafka-consumer-groups --reset-offsets --to-earliest --bootstrap-server broker:9092 --topic transactions --group connect-sink-crdb --execute

GROUP                          TOPIC                          PARTITION  NEW-OFFSET     
connect-sink-crdb              transactions                   0          0   
```

### API endpoints

```bash
# query connector status
curl -s http://localhost:8083/connectors/sink-crdb | jq '.'

# delete connector
curl -X DELETE http://localhost:8083/connectors/sink-crdb
```

## Reference

- [Confluent Kafka quick start](https://docs.confluent.io/platform/current/platform-quickstart.html#quick-start-for-cp)
- [Datagen Connector](https://docs.confluent.io/kafka-connectors/datagen/current/)
- [JDBC Sink Connector](https://docs.confluent.io/kafka-connectors/jdbc/current/sink-connector/overview.html)
- [JDBC Postresql Driver](https://jdbc.postgresql.org/documentation/use/)
